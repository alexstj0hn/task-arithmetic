{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Task Vectors — Phase 1 Training (Colab)\n",
    "\n",
    "Run on a free T4 GPU. Train one property at a time.\n",
    "\n",
    "**Before starting:** Runtime → Change runtime type → **T4 GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive (for persistent storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create persistent directory on Google Drive\n",
    "!mkdir -p /content/drive/MyDrive/protein-task-vectors/checkpoints\n",
    "!mkdir -p /content/drive/MyDrive/protein-task-vectors/zero_shot\n",
    "!mkdir -p /content/drive/MyDrive/protein-task-vectors/phase1_metrics\n",
    "!mkdir -p /content/drive/MyDrive/protein-task-vectors/task_vectors\n",
    "print('Google Drive mounted. Checkpoints will persist between sessions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone repo and install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CHANGE THIS to your GitHub repo URL\nREPO_URL = \"https://github.com/YOUR_USERNAME/task-arithmetic.git\"\n\nimport os\nif os.path.exists('/content/task-arithmetic'):\n    %cd /content/task-arithmetic\n    !git pull\nelse:\n    !git clone {REPO_URL} /content/task-arithmetic\n    %cd /content/task-arithmetic\n\n!pip install -e . -q\nprint('\\nDependencies installed.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MMseqs2\n",
    "!cd /tmp && wget -q https://mmseqs.com/latest/mmseqs-linux-avx2.tar.gz && tar xzf mmseqs-linux-avx2.tar.gz && cp mmseqs/bin/mmseqs /usr/local/bin/\n",
    "!mmseqs version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU\n",
    "import torch\n",
    "print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "print(f'VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\n",
    "print(f'bfloat16: {torch.cuda.is_bf16_supported()}')"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Create merged config with T4-safe settings\n# (base config assumes A100 80GB; T4 has only 16GB)\nimport yaml\n\nwith open('configs/train_config.yaml') as f:\n    config = yaml.safe_load(f)\n\nwith open('configs/colab_overrides.yaml') as f:\n    overrides = yaml.safe_load(f)\n\n# Deep merge overrides into config\nfor section, values in overrides.items():\n    if section in config and isinstance(config[section], dict):\n        config[section].update(values)\n    else:\n        config[section] = values\n\n# Write merged config\nwith open('configs/train_config_colab.yaml', 'w') as f:\n    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n\nprint('Created configs/train_config_colab.yaml with T4-safe settings:')\nprint(f'  mixed_precision: {config[\"training\"][\"mixed_precision\"]}')\nprint(f'  batch_size: {config[\"training\"][\"batch_size\"]}')\nprint(f'  list_size: {config[\"training\"][\"list_size\"]}')\nprint(f'  grad_accum: {config[\"training\"][\"gradient_accumulation_steps\"]}')\nprint(f'  eval_batch_size: {config[\"evaluation\"][\"eval_batch_size\"]}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Symlink results to Google Drive\n",
    "\n",
    "This way checkpoints survive Colab disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "DRIVE_DIR = '/content/drive/MyDrive/protein-task-vectors'\n",
    "REPO_DIR = '/content/task-arithmetic'\n",
    "\n",
    "# Symlink results subdirs to Google Drive\n",
    "for subdir in ['checkpoints', 'zero_shot', 'phase1_metrics', 'task_vectors']:\n",
    "    local = os.path.join(REPO_DIR, 'results', subdir)\n",
    "    remote = os.path.join(DRIVE_DIR, subdir)\n",
    "    if os.path.islink(local):\n",
    "        print(f'  {subdir}: already symlinked')\n",
    "    else:\n",
    "        if os.path.isdir(local):\n",
    "            # Copy any existing files first\n",
    "            for f in os.listdir(local):\n",
    "                src = os.path.join(local, f)\n",
    "                dst = os.path.join(remote, f)\n",
    "                if not os.path.exists(dst):\n",
    "                    shutil.copy2(src, dst) if os.path.isfile(src) else shutil.copytree(src, dst)\n",
    "            shutil.rmtree(local)\n",
    "        os.symlink(remote, local)\n",
    "        print(f'  {subdir}: symlinked to Drive')\n",
    "\n",
    "print('\\nResults will be saved to Google Drive automatically.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Download data\n",
    "\n",
    "Downloads ProteinGym (~500MB). Only runs once — skips if already downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.data.download --config configs/train_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Categorize and split (if not already done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('data/processed/category_assignments.json'):\n",
    "    !python -m src.data.categorize --config configs/train_config.yaml\n",
    "else:\n",
    "    print('Already categorized.')\n",
    "\n",
    "if not os.path.exists('data/splits/train_assays.json'):\n",
    "    !python -m src.data.splits --config configs/train_config.yaml\n",
    "else:\n",
    "    print('Splits already created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Zero-shot baseline\n",
    "\n",
    "Scores all assays with ESM-2 masked marginal likelihood.\n",
    "This takes ~2-4 hours for all 217 assays on T4. Skips already-scored assays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!python scripts/04_zero_shot.py --config configs/train_config_colab.yaml"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train property models\n",
    "\n",
    "Train ONE property per Colab session.\n",
    "Change `PROPERTY` below and run a new session for each.\n",
    "\n",
    "Order: stability → binding → expression → activity\n",
    "\n",
    "Each takes ~2-4 hours on T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# CHANGE THIS for each training session #\n",
    "#########################################\n",
    "PROPERTY = \"stability\"  # stability | binding | expression | activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!python scripts/05_train_property_models.py \\\n    --config configs/train_config_colab.yaml \\\n    --property {PROPERTY} \\\n    --resume"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate (after all 4 properties are trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Per-property evaluation\nfor prop in ['stability', 'binding', 'expression', 'activity']:\n    print(f'\\n=== Evaluating {prop} ===')\n    !python scripts/06_evaluate.py --config configs/train_config_colab.yaml --property {prop}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cross-property matrix (THE key result)\n!python scripts/06_evaluate.py --config configs/train_config_colab.yaml --cross-property"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the result\n",
    "import pandas as pd\n",
    "matrix = pd.read_csv('results/phase1_metrics/cross_property_matrix.csv', index_col=0)\n",
    "print('Cross-Property Evaluation Matrix (Spearman correlation)')\n",
    "print(matrix.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Extract task vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!python scripts/07_extract_vectors.py --config configs/train_config_colab.yaml"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View cosine similarity between task vectors\n",
    "import pandas as pd\n",
    "sim = pd.read_csv('results/task_vectors/cosine_similarity_matrix.csv', index_col=0)\n",
    "print('Task Vector Cosine Similarity')\n",
    "print(sim.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "All results are saved to your Google Drive at:\n",
    "- `My Drive/protein-task-vectors/checkpoints/` — trained models\n",
    "- `My Drive/protein-task-vectors/phase1_metrics/` — evaluation results\n",
    "- `My Drive/protein-task-vectors/task_vectors/` — extracted vectors\n",
    "\n",
    "You can close this notebook. Everything persists on Drive."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}